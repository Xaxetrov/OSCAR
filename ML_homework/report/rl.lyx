#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
% \documentclass[a4paper,10pt]{article}
\usepackage[english]{babel}
\usepackage{url}
\usepackage{subcaption}





% Training subcaption package to comply with
% IEEE standards. We can ignore the warning
% generated by caption.sty which is due to 
% the redefinition of \@makecaption
\DeclareCaptionLabelSeparator{periodspace}{.\quad}
\captionsetup{font=footnotesize,labelsep=periodspace,singlelinecheck=false}
\captionsetup[sub]{font=footnotesize,singlelinecheck=true}


\title{Machine Learning Homework 4:\\Markov Decision Processes}
\author{Nicolas Six}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding T1
\font_roman "lmodern" "default"
\font_sans "default" "default"
\font_typewriter "lmodern" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 10
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 0
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
maketitle
\end_layout

\end_inset

 
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
In this work we will explore solutions to solve problems that can be described
 as Markov Decision Processes (MDP).
 This sort of problems are very common and are generally found in games,
 but are not limited to them as lot of real life problems can be represented
 by this kind of approach.
 MDP is not always the best way to model a problem but it is interesting
 enough to be, nowadays, at the heart of artificial intelligence research.
 The goal of this report is not to present state of the art method but more
 to gain experience from now classic techniques.
\end_layout

\begin_layout Standard
We will here start by applying two well known algorithm to find a correct
 policy for two given problem.
 We will then explore the possibility to learn a policy directly from the
 problem, without needing any model from it.
\end_layout

\begin_layout Standard
We will use in this report the same convention as previously concerning
 plots.
 Unless stated otherwise, on each graphs, the hard line represent the median
 value of all the experiment and the hue area cover the space between min
 and max values.
\end_layout

\begin_layout Subsection
Problems
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
During this assignment we will study two different problems, or more exactly
 two different version of one famous problem.
 This MDP problem is the case of an agent playing at Starcraft II.
 You probably remember that our previous study were often grounded in Starcraft
 II, not that it is a game played, we just played one or two games to try
 it.
 If we were so interested in Starcraft II, it is because of an other project
 we started last semester with a group of friend to try to build an AI for
 Starcraft II using RL among other things.
 We will build our work here around it, and try to show some interesting
 aspect of this problem.
\end_layout

\begin_layout Standard
Since Deepmind created an AI capable of winning against the bests Go players
 in the world 
\begin_inset CommandInset citation
LatexCommand cite
key "AlphaGo0"

\end_inset

, Starcraft II is often referred as the new challenge for artificial intelligenc
e.
 This game is very interesting in an AI point of view as it mix together
 global strategy to defeat the enemy with low level micromanagement as well
 as a complex balancing system allowing a great number of possibilities.
 In addition, the number of possible state in this game far bigger than
 the one of Go and the action space is quite impressive too, the effect
 of the chosen action often have no direct impact and an agent must then
 wait a long time to the reward of the action it chose.
 This special combination of very short term reward and very long term one
 make that problem very challenging and let suppose plenty of real life
 application for the techniques developed to build this first step toward
 general artificial intelligence.
 This is why this problem is not only investigated by lot of research team,
 in the public field as well as in companies such as Google's Deepmind or
 Facebook research lead by Yann LeCun.
 A lot of bot contest for Starcraft already exist, but even the best bot
 of this competitions have no chance to win against moderate level human
 player, showing that this question remain open.
\end_layout

\begin_layout Standard
To allow the development of new bots, Blizzard (the Starcraft creator) and
 Deepmind released last summer an API, pysc2, that make the development
 of RL bots easier in Starcraft II.
 Among other things this API let us run the game on Linux without the GUI,
 allowing to run a full 20 minutes games in a couple of minutes.
 It may look fast but RL methods need a lot of examples, and some minutes
 are enough to transform learning session in hours.
\end_layout

\begin_layout Standard
As already stated above with a group of friends we used this API to build
 an artificial intelligence.
 Our goal was to build a general structure of multiple agent working together
 to build the whole AI.
 This approach is the one used in most of the actual bots, but is not the
 one presented by Deepmind in the paper accompanying the release of pysc2.
 Our goal is to cut the difficulty of the game into smaller problem that
 classic RL methods can solve.
 This project, which we named 
\begin_inset CommandInset href
LatexCommand href
name "OSCAR"
target "https://github.com/Xaxetrov/OSCAR"

\end_inset

, is still ongoing and is far from beating any common bot at this point,
 but provide a good ground to build our experience for this work.
\end_layout

\begin_layout Standard
OSCAR also gave a set of meta action allowing to build small chain of action
 to achieve a given task.
 For example, to build a barrack in Starcraft II, you need to select a worker
 and ask him to build a barrack in a free position available on the screen.
 A meta action will do the same sequence of action in the game point of
 view but will make it simpler for the agent, as it will not have to chose
 which worker select and were the barrack will be build.
\end_layout

\begin_layout Standard
In the learning point of view one of the advantage of OSCAR here is that
 it made all the structure available as an openAI Gym environment.
 It is then easy to plug into most of the available reinforcement learning
 algorithm available on the web.
 From this we build two specific environment trying to tackle the same problem,
 beat an original Starcraft AI on a very basic map.
\end_layout

\begin_layout Standard
The reward we chose are the same for both problems.
 A reward of 1.0 is given for a victory and 0.0 for a defeat, as the games
 are bounded in time, we also set a reward of 0.2 when no player win.
 We added to that some rewards to make our agent more offensive.
 We set a reward of 0.1 for killing at least an enemy unit in the last step
 and 0.2 for a building as well as a reward of 0.1 for creating a marine.
\end_layout

\begin_layout Subsubsection
Basic version
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "basicVersion"

\end_inset

 
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
The first version of the problem we will use here is pretty basic.
 The goal of this environment is to be the simplest possible will still
 allowing the AI to build an army and hopefully win.
\end_layout

\begin_layout Standard
The state we chose is a set of five boolean values, totaling 32 different
 possibilities.
 This state is composed of the information if at least a supply depot have
 been built (required to build a barrack), if at least a barrack have been
 built, if we have reached max population, if we have at least 10 soldiers
 and finally if the enemy base position is known.
 In the action point of view, we also selected very basic ones.
 The five meta action we chose are build a supply depot, build a barrack,
 train a marine (basic soldier), select the army and lastly do nothing.
 You can see that this set of action / state is very different from playing
 the full game, but will allow to build some soldiers and attack the enemy
 in a very basic fashion.
 A human playing this game will probably not enjoy it, as it will be like
 having five led in front of him and five possible button to use, with the
 additional difficulty for an agent that it does not have any memory in
 addition to the one provided by the five leds.
\end_layout

\begin_layout Standard
The main advantage of this version is to be very basic.
 With only 32 states and 5 actions, it will demonstrate the possibilities
 of classical approach to RL, and display their advantages.
\end_layout

\begin_layout Subsubsection
Complex version
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
The second version of our environment is a bit more complex.
 This problem not as basic as the previous one and allow the agent to have
 a better understanding of its state, but with few difference in terms of
 actions.
\end_layout

\begin_layout Standard
In this case the state is far bigger, with 2
\begin_inset space ~
\end_inset

982
\begin_inset space ~
\end_inset

525 different state possible.
 The main difference here is that the values are not boolean anymore but
 can take multiple values.
 In comparison to the basic version, the state now give the number of supply
 depot build in the range 0 to 22, the number of barrack from 0 to 4, the
 size of the army from 0 to 180 with step of 10, the number of worker collecting
 resources from 12 to 24 and an information on the time spend on this game
 simplified on 5 different steps.
 In terms of action this version has exactly the same than the one described
 in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "basicVersion"

\end_inset

, with the notable addition of an action to create new worker.
\end_layout

\begin_layout Standard
This version add a lot more possibilities, as for example basic economic
 management and far more information on the current game.
 This added information will allow a deterministic agent to build more complex
 strategies, but will also make it difficult to explore all the different
 possibilities.
 If this was designed to keep this number small enough to make value iteration
 possible, it will exhibit some of the limit of the classical approach which
 are value iteration and policy iteration.
\end_layout

\begin_layout Section
Solving MDP
\end_layout

\begin_layout Subsection
Introduction
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
A good way to find the best possible policy in a MDP context is to ether
 use value iteration or policy iteration, both of them will hopefully converge
 toward optimal policies.
 The problem of such algorithm is that they must know the entire set of
 possible state, transition and the underling reward and probabilities.
 It's probably not a problem in most of toys game used as example in reinforceme
nt learning, but can become much more problematic in the case of POMDP,
 of which games such as Starcraft are good example as the state of the enemy
 is mostly unknown by the agent.
\end_layout

\begin_layout Standard
To counter that problem and the fact that we do not have directly access
 to the state and transition of our problems, we decided to create a basic
 model of them.
 Those models take a lot of different assumption on the game and are not
 perfect representation of it.
 Due to this approximations, if we will fund polities that are optimal for
 our simplified version they will not be for the real environment.
\end_layout

\begin_layout Standard
The main assumption we made in our simplified environment is that each action
 as a direct effect on the state.
 For example, asking for training a marine give you one in the next state
 as well as attacking the enemy which directly in the next step kill all
 your army but may get you some rewards.
 Reward is also the major difference between the two problems.
 The basic one get the same reward when it attack with ten marines at the
 start of the game or at the end, because it as no idea of how long the
 game was, while in reality attacking with ten marines at end of game usually
 do very little damage to the enemy.
 That's also why we added a notion of time the complex version and simulated
 the grow of an enemy army.
\end_layout

\begin_layout Standard
An other approximation we made is that the enemy never attack, and so that
 you only lose our army when you attack and can not lose that game, which
 is not really a problem here as we defined a lost game as a null reward,
 the same as most of the possible actions.
\end_layout

\begin_layout Standard
This approximations of the environments can be viewed as domain knowledge,
 as they transform the game and rate it in a way that is personal to us.
 The way the probability of the transition is, for example, something that
 only an expert can do correctly.
 We tried here, with our modest experience in the game, to be as close as
 possible to the original environment, with our constraint on the state.
 This domain knowledge make the reinforcement learning part of value iteration
 and policy iteration here more a way to find the best policy out of the
 rules given by an expert.
 This problem is also very interesting and not that far from the original
 one.
\end_layout

\begin_layout Subsection
Value Iteration
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "vi"

\end_inset

 
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
Value iteration is the most stretch forward way to get the optimal policy
 from a perfect knowledge of transitions.
 We will try here to asses its results, particularly regarding the number
 of iteration needed to converge toward a policy.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{subfigure}[t]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

0.7
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
columnwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/all_value_iteration_episode_reward_env}.png
	width 110line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Reward get by the agent according to the iteration
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:vi_reward"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{subfigure}[t]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

0.7
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
columnwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/all_value_iteration_episode_steps_env}.png
	width 110line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Duration of a game in steps, according to the iteration
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:vi_steps"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{subfigure}[t]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

0.7
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
columnwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/all_value_iteration_win_state_env}.png
	width 110line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Victory / defeat state at end of game according to the iteration.
 0 is defeat, 1 null and 2 victory.
 The hard line represent here the mean value (and not the median as in other
 Figures)
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:vi_win"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Progress of the agent with the iteration of value algorithm
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:vi"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vi"

\end_inset

 show the agent results when using the policy get at a given iteration.
 For this set of test, we used a learning rate of 0.1 in both case without
 setting a 
\begin_inset Formula $\Delta$
\end_inset

 limit as we wanted the algorithm to try to improve the policy at each steps.
 We will study more in depth the effect of those parameters later on the
 is section.
\end_layout

\begin_layout Standard
You can see on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vi_reward"

\end_inset

 that environment converge toward similar results, which we must admit surprised
 us as we were expecting the basic environment to be too simple to get correct
 results.
 However, we can note that the complex version get correct result as soon
 as the first iteration and then change very slowly.
 In addition if we know that the basic version converge in the fourth iteration
 the complex version does not converge before the 19th and still update
 its policy for about 7000 actions on the five last step of this graph.
 Even if this is very small in comparison to the 
\begin_inset Formula $3\cdot10^{6}$
\end_inset

 state possible, particularly if you consider that most of this states are
 not possible in the game, it is still not negligible.
 However, this update are probably minor as they did not results into reward
 variation bigger than the underling noise we can observe on the basic case
 from iteration 3.
\end_layout

\begin_layout Standard
If you combine the results of Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vi_steps"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vi_win"

\end_inset

, you can see that, first the basic version lose quite quickly it's games
 and then converge to a policy allowing it to win some of its game, but
 mostly staying alive long enough to prevent loosing.
 While the complex version is more trying to end the game as quickly as
 possible, winning most of the time but not every times.
\end_layout

\begin_layout Standard
As we already stated previously the complexity of the environment have a
 big effect in terms of convergence toward a stable policy, but the variation
 are very small after the first iterations.
 Thus value iteration seams to be able to find good policy in few iterations,
 it was able to converge after three iterations on the basic problem.
 But if the complex problem gave good results since the first iterations,
 it takes 19 of them the reach a stable state.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{subfigure}[t]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

0.7
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
columnwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/value_gamma_iter}.png
	width 110line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Evolution of number of iterations to convergence according to gamma value
 for delta of 
\begin_inset Formula $10^{-}8$
\end_inset

 (delta of policy evaluation)
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:vi_param_gamma"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{subfigure}[t]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

0.7
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
columnwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/value_delta_iter}.png
	width 110line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Evolution of the number of iterations to convergence according to the delta
 value used for policy evaluation, with a gamma value set to 0.5
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:vi_param_delta"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Effect of the different of the different policy parameters on the convergence
 time
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:vi_param"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
You can see on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vi_param"

\end_inset

 the effect of the two main parameters of value iteration.
 We run this exploration only on the basic problem because as the complex
 problem's transition table does not fit into our RAM it will have taken
 days to do the same for this problem.
 It is easy to show that doing so will only give us similar results with
 larger convergence time as the two problems are very similar, the only
 difference being the number of states.
 The policy find was the same in every cases explored here.
 Apart from the cases when 
\begin_inset Formula $\gamma=0.0$
\end_inset

 and 
\begin_inset Formula $\gamma=1.0$
\end_inset

, where the policy was completely suboptimal because of horizons problems.
\end_layout

\begin_layout Standard
As you can see on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vi_param_gamma"

\end_inset

, in our case, the smaller 
\begin_inset Formula $\gamma$
\end_inset

 is the faster we get to the results.
 However, taking too small 
\begin_inset Formula $\gamma$
\end_inset

 values will quickly led to horizon problems.
 We suppose then that if relatively small values still work here it is because
 the optimal solution does not follow a very long path.
 But also because our transitions has a very small stochastic component.
 Thus we suppose that taking large 
\begin_inset Formula $\gamma$
\end_inset

 is more robust too complex problems but at the cost of computation time.
\end_layout

\begin_layout Standard
Considering the stopping condition the number of steps appear to be correlated
 to the log of the desired 
\begin_inset Formula $\Delta$
\end_inset

 between the value of two iterations.
 This result is displayed on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vi_param_delta"

\end_inset

.
 This is a good news as you can expect to get better results at minimal
 cost.
 However, we got here the same results every time, showing that our problem
 is simple enough to be solved in a very small number of iteration.
 You can note an outlier on graph for 
\begin_inset Formula $\Delta=10^{-3}$
\end_inset

, the only explanation we manage to find for it is related to the random
 value we use as a starting point.
\end_layout

\begin_layout Subsection
Policy Iteration
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "pi"

\end_inset

 
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
Policy iteration is very similar to value iteration, particularly when looking
 for an optimal policy.
 This similarity is particularly important in terms of results as both of
 them converge toward mostly identical policies.
\end_layout

\begin_layout Standard
We will here perform a similar study as in the previous section to see the
 impact of each iteration in terms of reward in the actual game as well
 as win / loss proportions and game duration.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{subfigure}[t]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

0.7
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
columnwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/all_policy_iteration_episode_reward_env}.png
	width 110line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Reward get by the agent according to the iteration
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:pi_reward"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{subfigure}[t]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

0.7
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
columnwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/all_policy_iteration_episode_steps_env}.png
	width 110line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Duration of a game in steps, according to the iteration
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:pi_steps"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{subfigure}[t]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

0.7
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
columnwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/all_policy_iteration_win_state_env}.png
	width 110line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Victory / defeat state at end of game according to the iteration.
 0 is defeat, 1 null and 2 victory.
 The hard line represent here the mean value (and not the median as in other
 Figures)
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:pi_win"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Progress of the agent with the iteration of policy algorithm
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:pi"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
As you can see on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:pi"

\end_inset

, both problems get very different results.
 As with value iteration the complex version get better results than the
 basic, while both of them are trying to solve the same problem, but with
 different tools.
\end_layout

\begin_layout Standard
On a general basis, both environment converge toward a stable policy in
 about two iterations, the variation are seen on the plot after this iteration
 are only due to the random aspect of the environment and the approximation
 made to construct the policy.
 To give precise numbers, for the basic version it converges at the second
 iteration while the complex problem it converges at the fifth step, but
 already have a very good approximation of this final policy at the second
 one.
\end_layout

\begin_layout Standard
On Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:pi_reward"

\end_inset

, you can see that the complex environment get high reward as soon as the
 first iteration.
 We did not expected it to converge that soon.
 Our supposition to explain that as it has far more states, the first see
 enough transitions to compute coherent policy.
 We can not also exclude the possibility of chance as we start with a random
 policy but given the number of state, it is quite unlikely.
 The basic version get bad results here, with a median reward being about
 half the one of the complex version.
 In addition we must note that it take longer to train it in terms of iteration
 as it have to wait the second iteration to see improvement in terms of
 reward.
\end_layout

\begin_layout Standard
In game duration point of view, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:pi_steps"

\end_inset

 show that the complex version manage to run the game during a longer time.
 It worth mentioning that if you merge this results with the one of Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:pi_win"

\end_inset

, not only the complex environment make the games finish later but it also
 win a lot of them, whereas the basic environment only lose most of them,
 and sometime manage, by chance, to win one.
 Showing again that optimizing our reward allow the agent to find a coherent
 policy for the real environment.
 If such a policy is not always enough to win, as in the case of the basic
 problem, you can still note the correlation between reward and win / defeat.
\end_layout

\begin_layout Standard
We can note here that the complexity in the dimentionnality of state space
 make the convergence a little longer.
 If the basic version converge in the second iteration, the complex version
 converge in the fifth.
 Considering that the complex environment as more than 90
\begin_inset space ~
\end_inset

000 time the number of state of the basic one, we can see that as a minor
 cost.
 Even if it must be noted that the iteration in the complex case are far
 longer than in the basic one.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{subfigure}[t]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

0.7
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
columnwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/policy_gamma_iter}.png
	width 110line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Evolution of number of iterations to convergence according to gamma value
 for delta of 
\begin_inset Formula $10^{-}8$
\end_inset

 (delta of policy evaluation)
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:pi_param_gamma"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{subfigure}[t]
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

0.7
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
columnwidth
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/policy_delta_iter}.png
	width 110line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Evolution of the number of iterations to convergence according to the delta
 value used for policy evaluation, with a gamma value set to 0.5
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:pi_param_delta"

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{subfigure}
\end_layout

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Effect of the different of the different policy parameters on the convergence
 time
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:pi_param"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
Policy iteration is quite stable when regarding the effect of its different
 parameters on convergence time.
 We explored on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:pi_param"

\end_inset

 the effect of them on the basic problem only, because as the transition
 table do not hold in RAM for the complex one, the process will have taken
 days on this last problem, just to get similar results.
\end_layout

\begin_layout Standard
As we said previously this algorithm is very stable, we did not fund any
 parameters that managed to change the time need to make it converge.
 Apart from the particular case of 
\begin_inset Formula $\gamma=0.0$
\end_inset

 and 
\begin_inset Formula $\gamma=1.0$
\end_inset

 where both the supposed optimal policy and the iteration number became
 meaning less due to horizon problems.
 In all the other cases policy iteration converge toward the same policy
 into four iterations no mater 
\begin_inset Formula $\gamma$
\end_inset

 value or the precision of the policy value estimation.
\end_layout

\begin_layout Subsection
Conclusion
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%why we didn't discussed wall time...
 (disk / ram)
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% compare policy of the two methods
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% compare speed
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% param effect: not so stocastic env
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% same resutls (testes on basic)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In conclusion, the two algorithms explored here do a good job in those cases.
 Both of them end up giving the same policy on the basic problem, this is
 not really the case for the complex problem, where after 20 iterations
 value iteration converge to a policy where 3
\begin_inset space ~
\end_inset

936 actions are still different from the policy get by policy iteration.
 However, this only represent 0.13% of difference in the policy, knowing
 that most of the state are probably possible but never appear on real game.
 At the end of the day, those two policies are very similar.
\end_layout

\begin_layout Standard
We must also note that in both case the link between reward and victory
 is not that clear, and good rewards be achieved without winning.
 This show that our reward definition is not perfect need some adjustment
 to let the agents win more games.
 Such a change that we will have tried given more time is to only use the
 final game reward as the only possible reward.
\end_layout

\begin_layout Standard
The main difference between them is time needed to converge.
 Value Iteration can take a lot of iterations to give it's results as we
 showed on section 
\begin_inset CommandInset ref
LatexCommand ref
reference "vi"

\end_inset

.
 However, those additional iteration did not change the final policy in
 our particular case.
 This is the reason why policy iteration finish in less iteration as it
 stop when policy do not evolve no matter if the underling value is exactly
 the one of the problem or not.
 If policy iteration is faster regarding the number of iterations it is
 not necessary the case in terms of wall time.
 For example, when the cost of reading the transition table is important,
 then evaluating the utility of a policy cost nearly the same as running
 value iteration.
 This was our case for the complex problem where the transition table must
 be written to disk.
\end_layout

\begin_layout Standard
When building the simplified version of the two problems to generate the
 transition table, we unconsciously reduced drastically the stochastic aspect
 of our problems.
 Making it easier to generate but also to learn from.
 This can be clearly seen through our parameters study as the policy remained
 correct with most of the parameters.
 If it difficult to be sure due to the noise of our results, we also suppose
 that the decreasing tendency you can see on both Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vi_reward"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:pi_reward"

\end_inset

 is due to an overfit of the policy to our model of the problem.
\end_layout

\begin_layout Standard
That two lasts points show us the limits of the techniques used here.
 If they work remarkably well, they need a perfect knowledge of the environment.
 Such knowledge is often really difficult to get from real system, in other
 case it often is too large to be usable and most of all it is often biased
 by the people constructing the model.
 If we discuss the usefulness of such a bias it is sometimes interesting
 to be able to learn without being constraint by the dogmatic knowledges.
\end_layout

\begin_layout Section
Dueling Q-Learning
\end_layout

\begin_layout Subsection
Introduction
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
The method we choose to use here is a particular version of the Q-learning
 algorithm witch was first presented by Wang et al.
 (2016) 
\begin_inset CommandInset citation
LatexCommand cite
key "DuelingQL"

\end_inset

.
 The original version is presented as part of Deep Q Network (DQN) but deep
 part of the network is mandatory.
\end_layout

\begin_layout Standard
We chosen to use for both problem a basic network build around three dense
 hidden layers of sixteen neurones each.
 While this structure stay very basic, it is probably complex enough to
 handle the complexity of both environments.
 State of the art methods manage to learn policies above the human level
 in days, as we plan here to test different set of parameters such training
 time is not conceivable.
 Even we can argue that our problem are far more simple than learning Atari
 game from pixels the time needed to generate the examples and the number
 of test we want to run, make time one of the biggest constraint here.
 Thus we decided to run our test on 100
\begin_inset space ~
\end_inset

000 steps, which is equivalent to 60 to 80 full games depending on the length
 of the games.
\end_layout

\begin_layout Subsection
Basic Idea of the algorithm
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
A dueling network is very close to a classic Q-Learning network in the way
 that both of them take the state as input and output the state-action value
 
\begin_inset Formula $Q$
\end_inset

.
 The particularity of the Dueling network is that instead of learning directly
 this 
\begin_inset Formula $Q$
\end_inset

 value for each state action pair, it decompose it as two different things,
 as you can see on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DuelingDQN"

\end_inset

.
 First the state value 
\begin_inset Formula $V(s)$
\end_inset

 which give the value of the current state.
 Second the advantage representing the benefit of taking a given action
 when in the current state, 
\begin_inset Formula $A(s,a)$
\end_inset

.
 This component are then simply used to compute the 
\begin_inset Formula $Q$
\end_inset

 value using the following equation: 
\begin_inset Formula 
\[
Q(s,a)=V(s)+A(s,a)
\]

\end_inset


\end_layout

\begin_layout Standard
The benefit of this method is that the advantage is common to all the action
 of a given state.
 This particularity allow the network to learn more easily that some state
 are good or bad no matter the action you chose, reducing the number of
 samples needed to learn a correct policy.
\end_layout

\begin_layout Standard
The other details on this architecture are out of scope here, but we recommend
 the interested reader to report to the original paper 
\begin_inset CommandInset citation
LatexCommand cite
key "DuelingQL"

\end_inset

 for more precision on the implementation as well as experimental result
 on Atari games.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename duelingDQN.png
	width 80line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Dueling architecture as presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "DuelingQL"

\end_inset

.
 Top network represent the classic DQN structure while the bottom one represent
 the Dueling structure
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:DuelingDQN"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Experiments
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
Our experiment where produced using the algorithms provided by 
\begin_inset CommandInset href
LatexCommand href
name "keras-rl"
target "https://github.com/keras-rl/keras-rl"

\end_inset

.
 But before to present our results, we need to clarify some implementation
 details.
\end_layout

\begin_layout Standard
First, the Q-Learning algorithm rely on a memory to remember the lasts observed
 transitions.
 This memory prevent the agent to overfit in the current state of the game
 and still have some examples of the experiences of previous games.
 We chose to set such a memory for the last 50
\begin_inset space ~
\end_inset

000 steps, which is more than 30 games.
 There are two different strategy to fill this memory before learning anything.
 The first is to play random action until the memory is filled and hope
 to have a correct representation of the states.
 The second is to fill it with examples and hope that the network will learn
 to copy them.
 The problem of this last case is that examples are not that easy to find
 in most case.
 Luckily, we have, tanks to the previous, section two agents that perform
 correctly for both problems.
 To have a better idea of the effect of such change in the train we explored
 the three different possibility: empty memory (overfit on the first observation
), random action memory and pretrained agent memory.
\end_layout

\begin_layout Standard
Second, as every reinforcement learning algorithm, Q-learning is subject
 to the exploration / exploitation dilemma.
 To try to its effect here we chose the following approach.
 We used a Botlzmann policy witch make all action possible even if some
 of them may have very small occurrence probability.
 We also increased this effect by setting the temperature parameter to high
 value in the first iteration and decreasing it toward 1.0 on a given number
 of step.
 Again we chose here to explore different value for this parameter.
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
As you can see on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:dqn_ml"

\end_inset

, the method used to fill the memory as no apparent impact on the reward
 the agent get from an episode.
 This showed to our surprise that does not take advantage of the experience
 we gave him.
 It really surprised us, as a paper from Cruz Jr et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "DQNpretraining"

\end_inset

 showed exactly the opposite result.
 This result also appear on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:dqn_ep_ml"

\end_inset

, where you can see that the noise as more importance here than this parameter.
 Thus, this kind of exploration strategy does not give the results we were
 looking for.
 Maybe a bigger memory and a longer training time will have given different
 results but we were not capable of producing such experiments.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/dqn_load_memory_episode_reward_env}.png
	width 80line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Evolution of the episode reward according to the method used to allocate
 memory.
 Agent mean that the memory was filled with experience from the previous
 section's agents, empty mean that nothing was stored in it, random mean
 that first random action were chosen in order to fill the memory.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:dqn_ml"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/dqn_episodes_episode_reward_load_memory_complex}.png
	width 80line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Evolution of the episode reward according to the episode number during training
 for different ways to fill replay memory.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:dqn_ep_ml"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
The number of step set for the exploration part of the training also showed
 very little importance in the final result.
 As displayed on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:dqn_em"

\end_inset

, this parameter appear to have no impact on the complex environment.
 However, we can see a variation of the maximum observed for the basic version.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:dqn_em_bs"

\end_inset

, show more details on that point.
 You can see that empty memory take advantage of the longer exploration,
 agent memory usefulness completely disappear with long exploration.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/dqn_exploration_lenght_episode_reward_env}.png
	width 80line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Evolution of the episode reward according to the number of step spend in
 linearly decreasing exploration mode.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:dqn_em"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/dqn_exploration_lenght_episode_reward_load_memory_basic}.png
	width 80line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Evolution of the episode reward according to the number of step spend in
 linearly decreasing exploration mode for different memory allocation on
 basic environment.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:dqn_em_bs"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
On more general basis, you can see on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:dqn_rw"

\end_inset

 that even if the result are noisy the learning agent managed to get a similar
 reward than the agent using the policy produced by value iteration or policy
 iteration.
 However, those result are not generalizable to the basic environment where
 the reward is far lower than the one observed previously.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../../../../Documents/Programation/Python/OSCAR/ML_homework/report/{../graphs/dqn_episodes_episode_reward_env}.png
	width 80line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
Evolution of the episode reward according to the episodes for both problems.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:dqn_rw"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Discussions
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
In conclusion, we can say that the results get in this section are not as
 good as the one we get in sections 
\begin_inset CommandInset ref
LatexCommand ref
reference "vi"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "pi"

\end_inset

.
 This is not really surprising as the informations given here are close
 to none.
 The domain knowledge greatly improve the quality of the result we get here
 and is the main difference between the two sections.
 In the first case we determined a policy uniquely from domain knowledge,
 never looking at the real system before to try our policy.
 In the second case we never looked for domain knowledge and only interacted
 with the environment to try to find something useful.
 We tried to build a bridge between the two methods by filling the replay
 memory with good example generated from domain knowledge, but it did not
 give the boost we were expecting.
\end_layout

\begin_layout Standard
As said previously we were not targeting state of the art method and so
 are convinced that RL can achieve far better results here.
 Such improvement can come from different factors, such as the description
 of the state given to the neural network.
 The learning algorithm used can also make a big difference such as illustrated
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "A3C"

\end_inset

.
 In addition with a large number of state as we have here it probably make
 sens to run the training on more steps.
 The 100
\begin_inset space ~
\end_inset

000 steps we used here, takes about two hours to run but are still more
 than an order of magnitude lower than the number of possible states.
 Some reflection can also be done around the reward as it as been shown
 that the this reward definition is not maximized when the agent win.
 We can imagine an agent which as learned to keep his enemy alive to be
 able kill more of its units and buildings, which will not help him it win.
 At the same time, the rewards were designed to make the learning proses
 easier for RL by giving it middle ground steps.
\end_layout

\begin_layout Standard
We also never discussed the fact that if the agent always play with the
 terran race, the enemy can be any of the three possible race: terran, zerg
 and protos.
 This races being very different from one to the other in terms of defense
 as well as in attack adding hidden parameters in the state.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Paragraph

\end_layout

\begin_layout Standard
During this work we explored different ways to find good policies when dealing
 with MDP.
 We first experimented with algorithms such as value iteration and policy
 iteration, and discovered that they managed to get good solution with relativel
y small need of computational time.
 But we also approached their limits, in terms of complexity due to the
 limited capability of a computer as well as in knowledge, as it is not
 always possible to have prior a perfect understanding of the problem which
 this method require.
\end_layout

\begin_layout Standard
In the second part we found that RL can overcome this two difficulties,
 with the example of a dueling Q-network.
 But if RL can pass those difficulties, it come with a cost, the one of
 being able to get a lot of example from the problems, if it is possible
 to do so with a small memory foot print it usually require lot of computation
 time.
 This time is manageable when you are dealing with computer generated experiment
, but when the process involve a human being or any real world experiment
 such approach are too slow at current state of science.
\end_layout

\begin_layout Standard
We must admit that we wanted to use Asynchronous Advantage Actor Critic
 
\begin_inset CommandInset citation
LatexCommand cite
key "A3C"

\end_inset

 instead of Q-network, but we did not manage to get a working implementation
 of this method on our problems.
 If A3C not the state of the art any more, it is still a large step above
 the DQN on which our work is based.
\end_layout

\begin_layout Standard
Our goal here was to gain as much experience as possible on MDP as well
 as on classic RL methods.
 We only regret that we did not have more time to go more in depth and try
 other modifications to both the problems and the algorithms and hope to
 have more time available soon to continue on this way.
\end_layout

\begin_layout Standard
You may have understood that we are particularly interested in that field,
 but we still tried to answer the questions given in the description and
 not only our own, as in the previous assignment.
 We tried here to mix both point of view to provide interesting information
 for the reader as well as valuable experience for us.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "rl"
options "default"

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
bibliographystyle{plain}
\end_layout

\end_inset

 
\end_layout

\end_body
\end_document
