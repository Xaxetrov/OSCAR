\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{AlphaGo0}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Problems}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}{section*.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Basic version}{2}{subsubsection.1.1.1}}
\newlabel{basicVersion}{{1.1.1}{2}{Basic version}{subsubsection.1.1.1}{}}
\@writefile{toc}{\contentsline {paragraph}{}{2}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Complex version}{2}{subsubsection.1.1.2}}
\@writefile{toc}{\contentsline {paragraph}{}{2}{section*.5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Solving MDP}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Introduction}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {paragraph}{}{2}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Value Iteration}{3}{subsection.2.2}}
\newlabel{vi}{{2.2}{3}{Value Iteration}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{}{3}{section*.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:vi_reward}{{1a}{3}{Reward get by the agent according to the iteration\relax }{figure.caption.8}{}}
\newlabel{sub@fig:vi_reward}{{a}{3}{Reward get by the agent according to the iteration\relax }{figure.caption.8}{}}
\newlabel{fig:vi_steps}{{1b}{3}{Duration of a game in steps, according to the iteration\relax }{figure.caption.8}{}}
\newlabel{sub@fig:vi_steps}{{b}{3}{Duration of a game in steps, according to the iteration\relax }{figure.caption.8}{}}
\newlabel{fig:vi_win}{{1c}{3}{Victory / defeat state at end of game according to the iteration. 0 is defeat, 1 null and 2 victory. The hard line represent here the mean value (and not the median as in other Figures)\relax }{figure.caption.8}{}}
\newlabel{sub@fig:vi_win}{{c}{3}{Victory / defeat state at end of game according to the iteration. 0 is defeat, 1 null and 2 victory. The hard line represent here the mean value (and not the median as in other Figures)\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Progress of the agent with the iteration of value algorithm\relax }}{3}{figure.caption.8}}
\newlabel{fig:vi}{{1}{3}{Progress of the agent with the iteration of value algorithm\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {paragraph}{}{4}{section*.10}}
\newlabel{fig:vi_param_gamma}{{2a}{4}{Evolution of number of iterations to convergence according to gamma value for delta of $10^{-}8$ (delta of policy evaluation)\relax }{figure.caption.9}{}}
\newlabel{sub@fig:vi_param_gamma}{{a}{4}{Evolution of number of iterations to convergence according to gamma value for delta of $10^{-}8$ (delta of policy evaluation)\relax }{figure.caption.9}{}}
\newlabel{fig:vi_param_delta}{{2b}{4}{Evolution of the number of iterations to convergence according to the delta value used for policy evaluation, with a gamma value set to 0.5\relax }{figure.caption.9}{}}
\newlabel{sub@fig:vi_param_delta}{{b}{4}{Evolution of the number of iterations to convergence according to the delta value used for policy evaluation, with a gamma value set to 0.5\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Effect of the different of the different policy parameters on the convergence time\relax }}{4}{figure.caption.9}}
\newlabel{fig:vi_param}{{2}{4}{Effect of the different of the different policy parameters on the convergence time\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Policy Iteration}{5}{subsection.2.3}}
\newlabel{pi}{{2.3}{5}{Policy Iteration}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{}{5}{section*.11}}
\newlabel{fig:pi_reward}{{3a}{5}{Reward get by the agent according to the iteration\relax }{figure.caption.12}{}}
\newlabel{sub@fig:pi_reward}{{a}{5}{Reward get by the agent according to the iteration\relax }{figure.caption.12}{}}
\newlabel{fig:pi_steps}{{3b}{5}{Duration of a game in steps, according to the iteration\relax }{figure.caption.12}{}}
\newlabel{sub@fig:pi_steps}{{b}{5}{Duration of a game in steps, according to the iteration\relax }{figure.caption.12}{}}
\newlabel{fig:pi_win}{{3c}{5}{Victory / defeat state at end of game according to the iteration. 0 is defeat, 1 null and 2 victory. The hard line represent here the mean value (and not the median as in other Figures)\relax }{figure.caption.12}{}}
\newlabel{sub@fig:pi_win}{{c}{5}{Victory / defeat state at end of game according to the iteration. 0 is defeat, 1 null and 2 victory. The hard line represent here the mean value (and not the median as in other Figures)\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Progress of the agent with the iteration of policy algorithm\relax }}{5}{figure.caption.12}}
\newlabel{fig:pi}{{3}{5}{Progress of the agent with the iteration of policy algorithm\relax }{figure.caption.12}{}}
\newlabel{fig:pi_param_gamma}{{4a}{6}{Evolution of number of iterations to convergence according to gamma value for delta of $10^{-}8$ (delta of policy evaluation)\relax }{figure.caption.13}{}}
\newlabel{sub@fig:pi_param_gamma}{{a}{6}{Evolution of number of iterations to convergence according to gamma value for delta of $10^{-}8$ (delta of policy evaluation)\relax }{figure.caption.13}{}}
\newlabel{fig:pi_param_delta}{{4b}{6}{Evolution of the number of iterations to convergence according to the delta value used for policy evaluation, with a gamma value set to 0.5\relax }{figure.caption.13}{}}
\newlabel{sub@fig:pi_param_delta}{{b}{6}{Evolution of the number of iterations to convergence according to the delta value used for policy evaluation, with a gamma value set to 0.5\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Effect of the different of the different policy parameters on the convergence time\relax }}{6}{figure.caption.13}}
\newlabel{fig:pi_param}{{4}{6}{Effect of the different of the different policy parameters on the convergence time\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{}{6}{section*.14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Conclusion}{6}{subsection.2.4}}
\citation{DuelingQL}
\citation{DuelingQL}
\citation{DuelingQL}
\citation{DQNpretraining}
\@writefile{toc}{\contentsline {section}{\numberline {3}Dueling Q-Learning}{7}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Introduction}{7}{subsection.3.1}}
\@writefile{toc}{\contentsline {paragraph}{}{7}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Basic Idea of the algorithm}{7}{subsection.3.2}}
\@writefile{toc}{\contentsline {paragraph}{}{7}{section*.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Dueling architecture as presented in \cite  {DuelingQL}. Top network represent the classic DQN structure while the bottom one represent the Dueling structure\relax }}{7}{figure.caption.17}}
\newlabel{fig:DuelingDQN}{{5}{7}{Dueling architecture as presented in \cite {DuelingQL}. Top network represent the classic DQN structure while the bottom one represent the Dueling structure\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experiments}{7}{subsection.3.3}}
\@writefile{toc}{\contentsline {paragraph}{}{7}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{}{7}{section*.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Evolution of the episode reward according to the method used to allocate memory. Agent mean that the memory was filled with experience from the previous section's agents, empty mean that nothing was stored in it, random mean that first random action were chosen in order to fill the memory.\relax }}{8}{figure.caption.20}}
\newlabel{fig:dqn_ml}{{6}{8}{Evolution of the episode reward according to the method used to allocate memory. Agent mean that the memory was filled with experience from the previous section's agents, empty mean that nothing was stored in it, random mean that first random action were chosen in order to fill the memory.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Evolution of the episode reward according to the episode number during training for different ways to fill replay memory.\relax }}{8}{figure.caption.21}}
\newlabel{fig:dqn_ep_ml}{{7}{8}{Evolution of the episode reward according to the episode number during training for different ways to fill replay memory.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {paragraph}{}{8}{section*.22}}
\@writefile{toc}{\contentsline {paragraph}{}{8}{section*.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Evolution of the episode reward according to the number of step spend in linearly decreasing exploration mode.\relax }}{8}{figure.caption.23}}
\newlabel{fig:dqn_em}{{8}{8}{Evolution of the episode reward according to the number of step spend in linearly decreasing exploration mode.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Evolution of the episode reward according to the number of step spend in linearly decreasing exploration mode for different memory allocation on basic environment.\relax }}{8}{figure.caption.24}}
\newlabel{fig:dqn_em_bs}{{9}{8}{Evolution of the episode reward according to the number of step spend in linearly decreasing exploration mode for different memory allocation on basic environment.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Discussions}{8}{subsection.3.4}}
\@writefile{toc}{\contentsline {paragraph}{}{8}{section*.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Evolution of the episode reward according to the episodes for both problems.\relax }}{8}{figure.caption.26}}
\newlabel{fig:dqn_rw}{{10}{8}{Evolution of the episode reward according to the episodes for both problems.\relax }{figure.caption.26}{}}
\citation{A3C}
\citation{A3C}
\bibstyle{plain}
\bibdata{rl}
\bibcite{DQNpretraining}{1}
\bibcite{A3C}{2}
\bibcite{AlphaGo0}{3}
\bibcite{DuelingQL}{4}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{9}{section.4}}
\@writefile{toc}{\contentsline {paragraph}{}{9}{section*.28}}
