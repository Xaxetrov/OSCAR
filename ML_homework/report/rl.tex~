% \documentclass[a4paper,10pt]{article}
\documentclass[twocolumn,a4paper,10pt]{article}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{url}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}

\geometry{a4paper,
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=20mm,
}


% Training subcaption package to comply with
% IEEE standards. We can ignore the warning
% generated by caption.sty which is due to 
% the redefinition of \@makecaption
\DeclareCaptionLabelSeparator{periodspace}{.\quad}
\captionsetup{font=footnotesize,labelsep=periodspace,singlelinecheck=false}
\captionsetup[sub]{font=footnotesize,singlelinecheck=true}


\title{Machine Learning Homework 4:\\Markov Decision Processes}
\author{Nicolas Six}

\makeatother

\begin{document}
\maketitle \tableofcontents{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

\paragraph{}

\subsection{Problems}
\paragraph{}

During this assignment we will study two different problems, or more exactly two different version of one famous problem. This MDP problem is the case of an agent playing at Starcraft II. You probably remember that our previous study were often grounded in Starcraft II, not that it is a game played, we just played one or two games to try it. If we were so interested in Starcraft II, it is because of an other project we started last semester with a group of friend to try to build an AI for Starcraft II using RL among other things. We will build our work here around it, and try to show some interesting aspect of this problem.

Since Deepmind created an AI capable of winning against the bests Go players in the world, Starcraft II is often referred as the new challenge for artificial intelligence. This game is very interesting in an AI point of view as it mix together global strategy to defeat the enemy with low level micromanagement as well as a complex a balancing system allowing a great number of possibilities. In addition, the number of possible state in this game far bigger than the one of Go and the action space is quite impressive too, the effect of the chosen action often have no direct impact and an agent must then wait a long time to the reward of the action it chose. This special combination of very short term reward and very long term one make that problem very challenging and let suppose plenty of real life application for the techniques developed to build this first step toward general artificial intelligence. This is why this problem is not only investigated by lot of research team, in the public field as well as in companies such as Google's Deepmind or Facebook research lead by Yann LeCun. A lot of bot contest for Starcraft already exist, but even the best bot of this competitions have no chance to win against moderate level human player, showing that this question remain open.

To allow the development of new bots, Blizzard (the Starcraft creator) and Deepmind released last summer an API, pysc2, that make the development of RL bots easier in Starcraft II. As already stated above with a group of friends we used this API to build an artificial intelligence. Our goal was to build a general structure of multiple agent working together to build the whole AI. This approach is the one used in most of the actual bots, but is not the one presented by Deepmind in the paper accompanying the release of pysc2. Our goal is to cut the difficulty of the game into smaller problem that classic RL methods can solve. This project, which we named \href{https://github.com/Xaxetrov/OSCAR}{OSCAR}, is still ongoing and is far from beating any common bot at this point, but provide a good ground to build our experience for this work.

OSCAR also gave a set of meta action allowing to build small chain of action to achieve a given task. For example, to build a barrack in Starcraft II, you need to select a worker and ask him to build a barrack in a free position available on the screen. A meta action will do the same sequence of action in the game point of view but will make it simpler for the agent, as it will not have to chose which worker select and were the barrack will be build.

In the learning point of view one of the advantage of OSCAR here is that it made all the structure available as a openAI Gym environment. It is then easy to plug into most of the available reinforcement learning algorithm available on the web. From this we build two specific environment trying to tackle the same problem, beat an original Starcraft AI on a very basic map.

The reward we chose are the same for both problems. A reward of 1.0 is given for a victory and 0.0 for a defeat, as the games are bounded in time, we also set a reward of 0.2 when no player win. We added to that some rewards to make our agent more offensive. We set a reward of 0.1 for killing at least an enemy unit in the last step and 0.2 for a building as well as a reward of 0.1 for creating a marines.

\subsubsection{Basic version} \label{basicVersion}
\paragraph{}
The first version of the problem we will use here is pretty basic. The goal of this environment is to be the simplest possible will still allowing the AI to build an army and hopefully win.

The state we chose is a set of five boolean values, totaling 32 different possibilities. This state is composed of the information if at least a supply depot have been built (required to build a barrack), if at least a barrack have been built, if we have reached max population, if we have at least 10 soldiers and finally if the enemy base position is known. In term of action, we also selected very basic ones. The five meta action we chose are build a supply depot, build a barrack, train a marines (basic soldier), select the army and lastly do nothing. You can see that this set of action / state is very different from playing the full game, but will allow to build some soldiers and attack the enemy in a very basic fashion. An human playing this game will probably not enjoy it as it will be like having five led in front of him and five possible button to use, with the additional difficulty for an agent that it does not have any memory in addition to the one provided by the five leds.

the main advantage of this version is to be very basic. With only 32 states and 5 actions, it will demonstrate the possibilities of classical approach to RL, and display their advantages.


\subsubsection{Complex version}
\paragraph{}
The second version of our environment is a bit more complex. This problem not as basic as the previous one and allow the agent to have a better understanding of its state, but with few difference in terms of actions.

In this case the state is far bigger, with 2~982~525 different state possible. The main difference here is that the values are not boolean anymore but can take multiple values. In comparison to the basic version, the state now give the number of supply depot build in the range 0 to 22, the number of barrack from 0 to 4, the size of the army from 0 to 180 with step of 10, the number of worker collecting resources from 12 to 24 and an information on the time spend on this game simplified on 5 different steps. In terms of action this version has exactly the same than the one described in section \ref{basicVersion}, with the notable addition of an action to create new worker.

This version add a lot more possibilities, as for example basic economic management and far more information on the current game. This added information will allow a deterministic agent to build more complex strategies, but will also make it difficult to explore all the different possibilities. If this was designed to keep this number small enough to make value iteration possible, it will exhibit some of the limit of the classical approach which are value iteration and policy iteration.

\section{Solving MDP}
\subsection{Introduction}
\paragraph{}
A good way to find the best possible policy in a MDP context is to ether use value iteration or policy iteration, both of them will converge toward the optimal policy. The problem of such algorithm is that they must know the entire set of possible state, transition and the underling reward and probabilities. It's probably not a problem in most of toys game used as example in reinforcement learning, but can become much more problematic in the case of POMDP, of which games such as Starcraft are good example as the state of the enemy is mostly unknown by the agent.

To counter that problem and the fact that we do not have directly access to the state and transition of our problems, we decided to create a basic model of them. Those models take a lot of different assumption on the game and are not perfect representation of it. Due to this approximations, if we will fund polities that are optimal for our simplified version they will not be for the real environment.

The main assumption we made in our simplified environment is that each action as a direct effect on the state. For example asking for training a marines give you one in the next state as well as attacking the enemy which directly in the next step kill all your army but may get you some rewards. Reward is also the major difference between the two problems. The basic one get the same reward when it attack with ten marines at the start of the game or at the end, because it as no idea of how long the game was, while in reality attacking with ten marines at end of game usually do very little damage to the enemy. That's also why we added a notion of time the complex version and simulated the grow of an enemy army.

An other approximation we made is that the enemy never attack, and so that you only lose our army when you attack and can not lose that game, which is not really a problem here as we defined a lost game as a null reward, the same as most of the possible actions.

This approximations of the environments can be viewed as domain knowledge, as they transform the game and rate it in a way that is personal to us. The way the probability of the transition is for example something that only an expert can do correctly. We tried here, with our modest experience in the game, to be as close as possible to the original environment, with our constraint on the state. This domain knowledge make the reinforcement learning part of value iteration and policy iteration here more a way to find the best policy out of the rules given by an expert. This problem is also very interesting and not that far from the original one.


\subsection{Value Iteration} \label{vi}
\paragraph{}
Value iteration is the most stretch forward way to get the optimal policy from a perfect knowledge of transitions. We will try here to asses its results, particularly regarding the number of iteration needed to converge toward a policy.

\begin{figure}
  \centering 
  \begin{subfigure}[t]{0.7\columnwidth} 
    \centering
    \includegraphics[width=1.1\linewidth]{{../graphs/all_value_iteration_episode_reward_env}.png}
    \caption{Reward get by the agent according to the iteration}
    \label{fig:vi_reward} 
  \end{subfigure} 
  \begin{subfigure}[t]{0.7\columnwidth}
    \centering 
    \includegraphics[width=1.1\linewidth]{{../graphs/all_value_iteration_episode_steps_env}.png}
    \caption{Duration of a game in steps, according to the iteration}
    \label{fig:vi_steps} 
  \end{subfigure} 
    \begin{subfigure}[t]{0.7\columnwidth}
    \centering 
    \includegraphics[width=1.1\linewidth]{{../graphs/all_value_iteration_win_state_env}.png}
    \caption{Victory / defeat state at end of game according to the iteration. 0 is defeat, 1 null and 2 victory. The hard line represent here the mean value (and not the median as in other Figures)}
    \label{fig:vi_win} 
  \end{subfigure} 
  \caption{Progress of the agent with the iteration of value algorithm}
\label{fig:vi} 
\end{figure}

Figure \ref{fig:vi} show the agent results when using the policy get at a given iteration. For this set of test, we used a learning rate of 0.1 in both case without setting a $\Delta$ limit as we wanted the algorithm to try to improve the policy at each steps. We will study more in depth the effect of those parameters later on the is section.

You can see on Figure \ref{fig:vi_reward} that environment converge toward similar results, which we must admit surprised us as we were expecting the basic environment to be too simple to get correct results. However we can note that the complex version get correct result as soon as the first iteration and then change very slowly. In addition if we know that the basic version converge in the fourth iteration the complex version does not converge in the ten first and still update its policy for about 7000 actions on the five last step. Even if this is very small in comparison to the $3\cdot10^6$ state possible, particularly if you consider that most of this states are not possible in the game, it is still not negligible. However this update are probably minor s they did not results into reward variation bigger than the underling noise we can observe on the basic case from iteration 3.

If you combine the results of Figures \ref{fig:vi_steps} and \ref{fig:vi_win}, you can see that, first the basic version lose quite quickly it's games and then converge to a policy allowing it to win some of its game, but mostly staying alive long enough to prevent loosing. While the complex version is more trying to end the game as quickly as possible, winning most of the time but not every times.

As we already stated previously the complexity of the environment have a big effect in terms of convergence toward a stable policy, but the variation are very small after the first iterations. Thus value iteration seams to be able to find good policy in few iterations, but is the problem is too complex it may take a large number of iteration for it to find the optimal policy. 

\begin{figure}
  \centering 
  \begin{subfigure}[t]{0.7\columnwidth} 
    \centering
    \includegraphics[width=1.1\linewidth]{{../graphs/value_gamma_iter}.png}
    \caption{Evolution of number of iterations to convergence according to gamma value for delta of $10^-8$ (delta of policy evaluation)}
    \label{fig:vi_param_gamma} 
  \end{subfigure} 
  \begin{subfigure}[t]{0.7\columnwidth}
    \centering 
    \includegraphics[width=1.1\linewidth]{{../graphs/value_delta_iter}.png}
    \caption{Evolution of the number of iterations to convergence according to the delta value used for policy evaluation, with a gamma value set to 0.5}
    \label{fig:vi_param_delta} 
  \end{subfigure} 
  \caption{Effect of the different of the different policy parameters on the convergence time}
\label{fig:vi_param} 
\end{figure}

\paragraph{}
You can see on Figure \ref{fig:vi_param} the effect of the two main parameters of value iteration. We run this exploration only on the basic problem because as the complex problem's transition table does not fit into our RAM it will have taken days to do the same for this problem. We expect that doing so will only give us similar results with larger convergence time as the two problems are very similar, the only difference being the number of states. The policy find was the same in every cases explored here. Apart from the cases when $\gamma=0.0$ and $\gamma=1.0$, where the policy was completely suboptimal because of horizons problems.

As you can see on Figure \ref{fig:vi_param_gamma}, in our case, the smaller $\gamma$ is the faster we get to the results. However, taking too small $\gamma$ values will quickly led to horizon problems. We suppose then that if relatively small values still work here it is because the optimal solution does not follow a very long path. But also because our transitions has a very small stochastic component. Thus we suppose that taking large $\gamma$ is more robust too complex problems but at the cost of computation time.

Considering the stopping condition the number of steps appear to be correlated to the log of the desired $\Delta$ between the value of two iterations. This result is displayed on Figure \ref{fig:vi_param_delta}. This is a good news as you can expect to get better results at minimal cost. However, we got here the same results every time, showing that our problem is simple enough to be solved in a very small number of iteration. You can note an outliers on graph for $\Delta = 10^{-3}$, the only explanation we manage to find for it is related to the random value we use as a starting point.

\subsection{Policy Iteration} \label{pi}
\paragraph{}
Policy iteration is very similar to value iteration, particularly when looking for an optimal policy. This similarity is particularly important in terms of results as both of them converge toward the same policy.

We will here perform a similar study as in the previous section to see the impact of each iteration in terms of reward in the actual game as well as win / loss proportions and game duration.

\begin{figure}
  \centering 
  \begin{subfigure}[t]{0.7\columnwidth} 
    \centering
    \includegraphics[width=1.1\linewidth]{{../graphs/all_policy_iteration_episode_reward_env}.png}
    \caption{Reward get by the agent according to the iteration}
    \label{fig:pi_reward} 
  \end{subfigure} 
  \begin{subfigure}[t]{0.7\columnwidth}
    \centering 
    \includegraphics[width=1.1\linewidth]{{../graphs/all_policy_iteration_episode_steps_env}.png}
    \caption{Duration of a game in steps, according to the iteration}
    \label{fig:pi_steps} 
  \end{subfigure} 
    \begin{subfigure}[t]{0.7\columnwidth}
    \centering 
    \includegraphics[width=1.1\linewidth]{{../graphs/all_policy_iteration_win_state_env}.png}
    \caption{Victory / defeat state at end of game according to the iteration. 0 is defeat, 1 null and 2 victory. The hard line represent here the mean value (and not the median as in other Figures)}
    \label{fig:pi_win} 
  \end{subfigure} 
  \caption{Progress of the agent with the iteration of policy algorithm}
\label{fig:pi} 
\end{figure}

As you can see on Figure \ref{fig:pi}, both problems get very different results. As with value iteration the complex version get better results than the basic, while both of them are trying to solve the same problem, but with different tools.

On a general basis, both environment converge toward the optimal policy in about two iterations, the variation are seen on the plot after this iteration are only due to the random aspect of the environment and the approximation made to construct the policy.

On Figure \ref{fig:pi_reward}, you can see that the complex environment get high reward as soon as the first iteration. We did not expected it to converge that soon. Our supposition to explain that as it has far more states, the first see enough transitions to compute coherent policy. We can not also exclude the possibility of chance as we start with a random policy but given the number of state, it is quite unlikely. The basic version get bad results here, with a median reward being about half the one of the complex version. In addition we must note that it take longer to train it in terms of iteration as it have to wait the second iteration to see improvement in terms of reward. 

In game duration point of view, Figure \ref{fig:pi_steps} show that the complex version manage to run the game during a longer time. It worth mentioning that if you merge this results with the one of Figure \ref{fig:pi_win}, not only the complex environment make the games finish later but it also win a lot of them, whereas the basic environment only lose most of them, and sometime manage, by chance, to win one. showing again that optimizing our reward allow the agent to find a coherent policy for the real environment. If such a policy is not always enough to win, as in the case of the basic problem, you can still note the correlation between reward and win / defeat.

%TODO: check convergence time of complex in policy iteration
We can note here that the the complexity in term of state does not make the convergence longer. According to our results, it is more the opposite result, the complex environment converge faster than the basic one when both of them have the same parameters.

\begin{figure}
  \centering 
  \begin{subfigure}[t]{0.7\columnwidth} 
    \centering
    \includegraphics[width=1.1\linewidth]{{../graphs/policy_gamma_iter}.png}
    \caption{Evolution of number of iterations to convergence according to gamma value for delta of $10^-8$ (delta of policy evaluation)}
    \label{fig:pi_param_gamma} 
  \end{subfigure} 
  \begin{subfigure}[t]{0.7\columnwidth}
    \centering 
    \includegraphics[width=1.1\linewidth]{{../graphs/policy_delta_iter}.png}
    \caption{Evolution of the number of iterations to convergence according to the delta value used for policy evaluation, with a gamma value set to 0.5}
    \label{fig:pi_param_delta} 
  \end{subfigure} 
  \caption{Effect of the different of the different policy parameters on the convergence time}
\label{fig:pi_param} 
\end{figure}

\paragraph{}
Policy iteration is quite stable when regarding the effect of its different parameters on convergence time. We explored on Figure \ref{fig:pi_param} the effect of them on the on the basic problem only, because as the transition table do not hold in RAM for the complex one, the process will have taken days on this last problem, just to get similar results.

As we said previously this algorithm is very stable, we did not fund any parameters that managed to change the time need to make it converge. Apart from the particular case of $\gamma=0.0$ and $\gamma=1.0$ where both the supposed optimal policy and the iteration number became meaning less due to horizon problems. In all the other cases policy iteration converge toward the same policy into four iterations no mater $\gamma$ value or the precision of the policy value estimation.

\subsection{Conclusion}
%why we didn't discussed wall time... (disk / ram)
% compare policy of the two methods
% compare speed
% param effect: not so stocastic env
% same resutls (testes on basic)

In conclusion, the two algorithms explored here do a good job in those cases. Both of them up giving the same policy on the basic problem.%TODO: and on the complex one ?

The main difference between them is time needed to converge. Value Iteration can take a lot of iterations to give it's results as we showed on section \ref{vi}. However, those additional iteration did not change the final policy in our particular case. This is the reason why policy iteration finish in less iteration as it stop when policy do not evolve no matter if the underling value is exactly the one of the problem or not. If policy iteration is faster in term of iteration it is not necessary the case in terms of wall time. For example when the cost of reading the transition table is important, then evaluating the utility of a policy cost nearly the same as running value iteration. This was our case for the complex problem where the transition table must be written to disk.

When building the simplified version of the two problems to generate the transition table, we unconsciously reduced drastically the stochastic aspect of our problems. Making it easier to generate but also to learn from. This can be clearly seen through our parameters study as the policy remained correct with most of the parameters. If it difficult to be sure due to the noise of our results, we also suppose that the decreasing tendency you can see on both Figures \ref{fig:vi_reward} and \ref{fig:pi_reward} is due to an overfit of the policy to our model of the problem.

That two lasts points show us the limits of the techniques used here. If they work remarkably well, they need a perfect knowledge of the environment. Such knowledge is often really difficult to get from real system, in other case it often is too large to be usable and most of all it is often biased by the people constructing the model. If we discuss the usefulness of such a bias it is sometimes interesting to be able to learn without being constraint by the dogmatic knowledges.

\section{Asynchronous Advantage Actor Critic}
\subsection{Introduction}


\subsection{Basic Idea of the algorithm}


\subsection{Experiments}

\section{Conclusion}

\end{document}
